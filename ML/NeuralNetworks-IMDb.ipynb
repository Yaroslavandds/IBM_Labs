{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import software libraries and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys                             # Read system parameters.\n",
    "import shutil\n",
    "import numpy as np                     # Work with multi-dimensional arrays and matrices.\n",
    "from numpy.random import seed\n",
    "import matplotlib as mpl               # Create 2D charts.\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn                         # Perform data mining and analysis.\n",
    "import tensorflow                      # Train neural networks for deep learning.\n",
    "import keras                           # Provide a frontend for TensorFlow.\n",
    "from keras import datasets\n",
    "\n",
    "# Summarize software libraries used.\n",
    "print('Libraries used in this project:')\n",
    "print('- Python {}'.format(sys.version))\n",
    "print('- NumPy {}'.format(np.__version__))\n",
    "print('- Matplotlib {}'.format(mpl.__version__))\n",
    "print('- scikit-learn {}'.format(sklearn.__version__))\n",
    "print('- TensorFlow {}'.format(tensorflow.__version__))\n",
    "print('- Keras {}\\n'.format(keras.__version__))\n",
    "\n",
    "# Load the dataset.\n",
    "shutil.rmtree('/home/jovyan/.keras')\n",
    "shutil.copytree('/home/jovyan/work/.keras', '/home/jovyan/.keras')\n",
    "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words = 10000)\n",
    "print('Loaded {} training records.'.format(len(X_train.data)))\n",
    "print('Loaded {} test records.'.format(len(X_test.data)))\n",
    "\n",
    "# Uncomment the following two lines to make outcomes deterministic. Supply whatever seed values you wish.\n",
    "#seed(1)\n",
    "#tensorflow.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get acquainted with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First example features:\\n')\n",
    "print(X_train[0])\n",
    "print('\\n')\n",
    "print('Label: {}'.format(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode sequence values into actual text.\n",
    "index = datasets.imdb.get_word_index()\n",
    "index_dict = dict([(value, key) for (key, value) in index.items()])\n",
    "decode = ' '.join([index_dict.get(i - 3, '?') for i in X_train[0]])  # Replace unknown words with '?'.\n",
    "print(decode) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine some statistics about the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [len(i) for i in X_train]\n",
    "print('Mean review length (in words): {:.0f}'.format(np.mean(result)))\n",
    "print('Standard deviation (in words): {:.0f}'.format(np.std(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 2))\n",
    "plt.boxplot(result, vert = False)\n",
    "plt.xlabel('Review length (in words)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add padding to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = 500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = 500)\n",
    "\n",
    "print('Number of features: {}'.format(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the training and validation datasets and their labels.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state = 50)\n",
    "\n",
    "print(f'Training features:         {X_train.shape}')\n",
    "print(f'Validation features:       {X_val.shape}')\n",
    "print(f'Training labels:           {y_train.shape}')\n",
    "print(f'Validation labels:         {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the RNN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "rnn = Sequential()\n",
    "\n",
    "# Start stacking layers one-by-one.\n",
    "rnn.add(Embedding(input_dim = 10000,  # Size of vocabulary (top 10,000 words).\n",
    "                  output_dim = 100,  # 100-dimensional vector embedding.\n",
    "                  input_length = 500))  # Length of review (in words).\n",
    "\n",
    "rnn.add(LSTM(units = 64))  # 64-dimensional LSTM.\n",
    "rnn.add(LeakyReLU(alpha = 0.1))\n",
    "\n",
    "rnn.add(Dense(128, activation = 'linear'))\n",
    "rnn.add(LeakyReLU(alpha = 0.1))\n",
    "rnn.add(Dense(1, activation = 'sigmoid'))  # Dense output layer with sigmoid activation.\n",
    "\n",
    "print('The RNN structure has been built.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model and examine the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(optimizer = 'adam',\n",
    "            loss = 'binary_crossentropy',\n",
    "            metrics = ['accuracy'])\n",
    "\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required library.\n",
    "!conda install --yes graphviz==2.40.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(rnn, show_shapes = True, to_file = 'model2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_trained = rnn.fit(X_train, y_train,\n",
    "                      validation_data = (X_val, y_val),\n",
    "                      epochs = 1,\n",
    "                      verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_test = rnn.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "print('Loss: {}'.format(round(eval_test[0], 2)))\n",
    "print('Accuracy: {:.0f}%'.format(eval_test[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rnn.predict(X_test[:100])  # Predict first 100 examples to save time.\n",
    "prediction = np.round(np.ndarray.flatten(prediction))  # Round probabilities to 0 or 1.\n",
    "prediction = prediction.astype(int)\n",
    "\n",
    "print('Actual class:    {}'.format(y_test[:10]))\n",
    "print('Predicted class: {}'.format(prediction[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine a review that was correctly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['negative', 'positive']\n",
    "\n",
    "for i in range(100):\n",
    "    if y_test[i] == prediction[i]:\n",
    "        print('Actual review sentiment:    {}'.format(label_names[y_test[i]]))\n",
    "        print('Predicted review sentiment: {}\\n'.format(label_names[prediction[i]]))\n",
    "        \n",
    "        decode = ' '.join([index_dict.get(x - 3, '?') for x in X_test[i]])\n",
    "        \n",
    "        print(decode)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine a review that was incorrectly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    if y_test[i] != prediction[i]:\n",
    "        print('Actual review sentiment:    {}'.format(label_names[y_test[i]]))\n",
    "        print('Predicted review sentiment: {}\\n'.format(label_names[prediction[i]]))\n",
    "        \n",
    "        decode = ' '.join([index_dict.get(x - 3, '?') for x in X_test[i]])\n",
    "        \n",
    "        print(decode)\n",
    "        \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
