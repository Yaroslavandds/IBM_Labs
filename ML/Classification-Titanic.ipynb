{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import software libraries and load the dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys                             # Read system parameters.\n",
    "import os                              # Interact with the operating system.\n",
    "import numpy as np                     # Work with multi-dimensional arrays and matrices.\n",
    "import pandas as pd                    # Manipulate and analyze data.\n",
    "import matplotlib                      # Create 2D charts.\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn                         # Perform data mining and analysis.\n",
    "from time import time                  # Calculate training time.\n",
    "\n",
    "# Summarize software libraries used.\n",
    "print('Libraries used in this project:')\n",
    "print('- Python {}'.format(sys.version))\n",
    "print('- NumPy {}'.format(np.__version__))\n",
    "print('- pandas {}'.format(pd.__version__))\n",
    "print('- Matplotlib {}'.format(matplotlib.__version__))\n",
    "print('- scikit-learn {}\\n'.format(sklearn.__version__))\n",
    "\n",
    "# Load the dataset.\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT_DIR, \"titanic_data\")\n",
    "print('Data files in this project:', os.listdir(DATA_PATH))\n",
    "data_raw_file = os.path.join(DATA_PATH, 'train.csv')\n",
    "data_raw = pd.read_csv(data_raw_file)\n",
    "print('Loaded {} records from {}.'.format(len(data_raw), data_raw_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get acquainted with the dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_raw.info())      # View data types and see if there are missing entries.\n",
    "data_raw.head(10)           # View first 10 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine a general summary of statistics #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('float_format', '{:.2f}'.format): \n",
    "    print(data_raw.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use stacked bar visualization to show survival numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_list = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
    "\n",
    "for feature in feature_list:\n",
    "    plot_set = data_raw.groupby([feature, 'Survived'])\n",
    "    plot_set = plot_set.size().reset_index()\n",
    "    plot_set = plot_set.pivot(columns=feature,index='Survived',values=0)\n",
    "    plot_set.plot(kind='bar', stacked=True, rot=0, figsize=(20,3)).set_title(f'Survival Rates by {feature}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for relationships between survival, age, and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "survived = 'Survived'\n",
    "perished = 'Perished'\n",
    "\n",
    "men = data_raw[data_raw['Sex']=='male'].round(0)\n",
    "women = data_raw[data_raw['Sex']=='female'].round(0)\n",
    "\n",
    "women_plot = women.groupby(['Survived','Age']).size().reset_index().pivot(columns='Survived', index='Age', values=0)\n",
    "ax = women_plot.plot(kind='bar', stacked=True, figsize=(20,4), title='Female Survival by Age')\n",
    "ax.set_xlim(0, 80)\n",
    "ax.set_ylim(0, 20)\n",
    "\n",
    "men_plot = men.groupby(['Survived','Age']).size().reset_index().pivot(columns='Survived', index='Age', values=0)\n",
    "ax = men_plot.plot(kind='bar', stacked=True, figsize=(20,4), title='Male Survival by Age')\n",
    "ax.set_xlim(0, 80)\n",
    "ax.set_ylim(0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of missing values:\\n{}\\n'.format(data_raw.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate training and test sets already exist.\n",
    "# A validation set will be split off from the training sets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 'Survived' is the dependent variable (value to be predicted), so it will be\n",
    "# removed from the training data and put into a separate DataFrame for labels.\n",
    "label_columns = ['Survived']\n",
    "\n",
    "training_columns = ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
    "\n",
    "# Split the training and validation datasets and their labels.\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_raw[training_columns],\n",
    "                                                                            data_raw[label_columns],\n",
    "                                                                            random_state = 1912)\n",
    "\n",
    "# Compare the number of rows and columns in the original data to the training and validation sets.\n",
    "print(f'Original set:        {data_raw.shape}')\n",
    "print('------------------------------')\n",
    "print(f'Training features:   {X_train.shape}')\n",
    "print(f'Validation features: {X_val.shape}')\n",
    "print(f'Training labels:     {y_train.shape}')\n",
    "print(f'Validation labels:   {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify columns that should be modified or deleted from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine how to handle ticket values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_raw.Ticket.sort_values().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify all personal titles and embarked port codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = data_raw['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "print('Titles: ', title.unique())\n",
    "\n",
    "embarked_loc = data_raw['Embarked']\n",
    "print('Embarked locations: ', embarked_loc.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform common preparation on the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform common cleaning and feature engineering tasks on datasets.\n",
    "def prep_dataset(dataset):\n",
    "    \n",
    "    print('Before prep:\\n\\n{}\\n'.format(dataset.isnull().sum())) \n",
    "    \n",
    "    # PROVIDE MISSING VALUES\n",
    "    \n",
    "    # Fill missing Age values with the median age.\n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n",
    "\n",
    "    # Fill missing Fare values with the median fare.\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n",
    "\n",
    "    # Fill missing Embarked values with the mode.\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "    \n",
    "    \n",
    "    # FEATURE ENGINEERING\n",
    "    \n",
    "    # Size of family and whether passenger is traveling alone.\n",
    "    size_of_family = dataset['SibSp'] + dataset['Parch'] + 1        \n",
    "    dataset['SizeOfFamily'] = size_of_family\n",
    "        \n",
    "    # Extract the title from the passenger's name.\n",
    "    title = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "    dataset['Title'] = title\n",
    "    \n",
    "    \n",
    "    # CONVERT CATEGORICAL VALUES TO NUMERIC ENCODINGS\n",
    "    \n",
    "    title_encode = {}\n",
    "    count = 1\n",
    "    for i in title.unique():\n",
    "        title_encode.update({i: count})\n",
    "        count += 1\n",
    "    \n",
    "    sex_encode = {'female': 1, 'male': 2}\n",
    "    \n",
    "    embarked_encode = {'S':1, 'C':2, 'Q':3}\n",
    "    \n",
    "    dataset['SexEncoding'] = dataset['Sex'].map(sex_encode)\n",
    "    dataset['EmbarkedEncoding'] = dataset['Embarked'].map(embarked_encode)\n",
    "    \n",
    "    dataset['TitleEncoding'] = dataset['Title'].map(title_encode)\n",
    "    dataset['TitleEncoding'].fillna(dataset['TitleEncoding'].mode()[0], inplace = True)\n",
    "        \n",
    "    print('After prep:\\n\\n{}\\n'.format(dataset.isnull().sum()))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "print('---- TRAINING -----')\n",
    "X_train = prep_dataset(X_train.copy())\n",
    "\n",
    "print('---- VALIDATION -----')\n",
    "X_val = prep_dataset(X_val.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview current training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop columns that won't be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused columns from datasets.\n",
    "def drop_unused(dataset):\n",
    "    \n",
    "    print('Columns before drop:\\n\\n{}\\n'.format(list(dataset.columns)))\n",
    "        \n",
    "    dataset = dataset.drop(['PassengerId'], axis=1)\n",
    "    dataset = dataset.drop(['Cabin'], axis=1)\n",
    "    dataset = dataset.drop(['Ticket'], axis=1)\n",
    "    dataset = dataset.drop(['Name'], axis=1)\n",
    "\n",
    "    # These have been replaced with numeric codes.\n",
    "    dataset = dataset.drop(['Title'], axis=1)\n",
    "    dataset = dataset.drop(['Sex'], axis=1)\n",
    "    dataset = dataset.drop(['Embarked'], axis=1)\n",
    "    \n",
    "    print('Columns after drop:\\n\\n{}\\n'.format(list(dataset.columns)))\n",
    "    return dataset\n",
    "\n",
    "print('---- TRAINING -----')\n",
    "X_train = drop_unused(X_train.copy())\n",
    "\n",
    "print('--- VALIDATION ----')\n",
    "X_val = drop_unused(X_val.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(solver='sag', C = 0.05, max_iter = 10000)\n",
    "start = time()\n",
    "log_reg.fit(X_train, np.ravel(y_train))\n",
    "end = time()\n",
    "train_time = (end - start) * 1000\n",
    "\n",
    "# Score using the validation data.\n",
    "score = log_reg.score(X_val, y_val)\n",
    "\n",
    "print('Logistic regression model took {:.2f} milliseconds to fit.'.format(train_time))\n",
    "print('Score on validation set: {:.0f}%'.format(score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use validation set to evaluate.\n",
    "results_comparison = X_val.copy()\n",
    "results_comparison['PredictedSurvival'] = log_reg.predict(X_val)\n",
    "results_comparison['ActualSurvival'] = y_val.copy()\n",
    "results_comparison['ProbPerished'] = np.round(log_reg.predict_proba(X_val)[:, 0] * 100, 2)\n",
    "results_comparison['ProbSurvived'] = np.round(log_reg.predict_proba(X_val)[:, 1] * 100, 2)\n",
    "\n",
    "# View examples of the predictions compared to actual survival.\n",
    "results_comparison.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a *k*-nearest neighbor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Use bootstrapping to find ideal k value.\n",
    "k_num = round(sqrt(X_train.shape[0]))\n",
    "\n",
    "# Make k odd if it is even.\n",
    "if k_num % 2 == 0:\n",
    "    k_num += 1\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train model.\n",
    "knn = KNeighborsClassifier(n_neighbors = k_num) \n",
    "start = time()\n",
    "knn.fit(X_train, np.ravel(y_train))  \n",
    "end=time()\n",
    "train_time = (end - start) * 1000\n",
    "\n",
    "# Score using the validation data.\n",
    "score = knn.score(X_val, y_val)\n",
    "\n",
    "print('Value of k: {}'.format(k_num))\n",
    "print('KNN model took {:.2f} milliseconds to fit.'.format(train_time))\n",
    "print('Score on validation set: {:.0f}%'.format(score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use validation set to evaluate.\n",
    "results_comparison = X_val.copy()\n",
    "results_comparison['PredictedSurvival'] = knn.predict(X_val)\n",
    "results_comparison['ActualSurvival'] = y_val.copy()\n",
    "results_comparison['ProbPerished'] = np.round(knn.predict_proba(X_val)[:, 0] * 100, 2)\n",
    "results_comparison['ProbSurvived'] = np.round(knn.predict_proba(X_val)[:, 1] * 100, 2)\n",
    "\n",
    "# View examples of the predictions compared to actual survival.\n",
    "results_comparison.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the logistic regression model to make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the test dataset.\n",
    "X_test_file = os.path.join(DATA_PATH, 'test.csv')\n",
    "X_test_raw = pd.read_csv(X_test_file)\n",
    "print('Loaded {} records from {}\\n'.format(len(X_test_raw), X_test_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset and drop unneeded columns.\n",
    "print('Preparing test data for prediction\\n')\n",
    "X_test = prep_dataset(X_test_raw.copy())\n",
    "X_test = drop_unused(X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example predictions with the original test data.\n",
    "results_log_reg = X_test_raw.copy()\n",
    "results_log_reg['PredictedSurvival'] = log_reg.predict(X_test)\n",
    "results_log_reg['ProbPerished'] = np.round(log_reg.predict_proba(X_test)[:, 0] * 100, 2)\n",
    "results_log_reg['ProbSurvived'] = np.round(log_reg.predict_proba(X_test)[:, 1] * 100, 2)\n",
    "results_log_reg.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
