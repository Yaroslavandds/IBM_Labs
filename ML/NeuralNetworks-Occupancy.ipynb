{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import software libraries and load the dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries used in this project:\n",
      "- Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) \n",
      "[GCC 7.3.0]\n",
      "- NumPy 1.16.2\n",
      "- pandas 0.24.2\n",
      "- Matplotlib 3.0.3\n",
      "- scikit-learn 0.20.3\n",
      "\n",
      "Data files in this project: ['train.csv', 'test.csv']\n",
      "Loaded 8143 records from ./occupancy_data/train.csv.\n",
      "Loaded 2665 records from ./occupancy_data/test.csv.\n"
     ]
    }
   ],
   "source": [
    "import sys                             # Read system parameters.\n",
    "import os                              # Interact with the operating system.\n",
    "import numpy as np                     # Work with multi-dimensional arrays and matrices.\n",
    "import pandas as pd                    # Manipulate and analyze data.\n",
    "import matplotlib as mpl               # Create 2D charts.\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn                         # Perform data mining and analysis.\n",
    "from sklearn.utils import shuffle\n",
    "import VisualizeNN as VisNN            # Create neural network visualizations.\n",
    "from time import time                  # Calculate training time.\n",
    "\n",
    "# Summarize software libraries used.\n",
    "print('Libraries used in this project:')\n",
    "print('- Python {}'.format(sys.version))\n",
    "print('- NumPy {}'.format(np.__version__))\n",
    "print('- pandas {}'.format(pd.__version__))\n",
    "print('- Matplotlib {}'.format(mpl.__version__))\n",
    "print('- scikit-learn {}\\n'.format(sklearn.__version__))\n",
    "\n",
    "# Load the dataset.\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT_DIR, \"occupancy_data\")\n",
    "print('Data files in this project:', os.listdir(DATA_PATH))\n",
    "data_raw_file_train = os.path.join(DATA_PATH, 'train.csv')\n",
    "data_raw_file_test = os.path.join(DATA_PATH, 'test.csv')\n",
    "data_raw = pd.read_csv(data_raw_file_train)\n",
    "data_raw_test = pd.read_csv(data_raw_file_test)\n",
    "print('Loaded {} records from {}.'.format(len(data_raw), data_raw_file_train))\n",
    "print('Loaded {} records from {}.'.format(len(data_raw_test), data_raw_file_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get acquainted with the dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset.\n",
    "data_raw = shuffle(data_raw.copy(), random_state = 765)\n",
    "data_raw.reset_index(inplace = True, drop = True)\n",
    "\n",
    "data_raw_test = shuffle(data_raw_test.copy(), random_state = 765)\n",
    "data_raw_test.reset_index(inplace = True, drop = True)\n",
    "\n",
    "print(data_raw.info())      # View data types and see if there are missing entries.\n",
    "data_raw.head(10)           # View first 10 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the distribution of various features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Matplotlib to plot figures.\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "data_raw.hist(figsize=(20,15));\n",
    "plt.figure();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine a general summary of statistics #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('float_format', '{:.3f}'.format): \n",
    "    print(data_raw.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the label from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate training and test sets already exist.\n",
    "\n",
    "# 'Occupancy' is the dependent variable (value to be predicted), so it will be\n",
    "# removed from the training and testing data and put into a separate DataFrame for labels.\n",
    "label_columns = ['Occupancy']\n",
    "\n",
    "training_columns = ['Date', 'Temperature', 'RelativeHumidity', 'Light', 'CO2', 'HumidityRatio']\n",
    "\n",
    "# Split the training and test datasets and their labels.\n",
    "X_train, y_train = data_raw[training_columns].copy(), data_raw[label_columns].copy()\n",
    "X_test, y_test = data_raw_test[training_columns].copy(), data_raw_test[label_columns].copy()\n",
    "\n",
    "# Compare the number of rows and columns in the original data to the training and test sets.\n",
    "print(f'Original set:        {data_raw.shape}')\n",
    "print('------------------------------')\n",
    "print(f'Training features:   {X_train.shape}')\n",
    "print(f'Test features:       {X_test.shape}')\n",
    "print(f'Training labels:     {y_train.shape}')\n",
    "print(f'Test labels:         {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the `Date` column to datetime format for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Date'] = pd.to_datetime(X_train['Date'])\n",
    "X_test['Date'] = pd.to_datetime(X_test['Date'])\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine which datetime components have unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific datetime components and retrieve unique values.\n",
    "print('Unique years:   {}'.format(X_train['Date'].dt.year.unique()))\n",
    "print('Unique months:  {}'.format(X_train['Date'].dt.month.unique()))\n",
    "print('Unique days:    {}'.format(X_train['Date'].dt.day.unique()))\n",
    "print('Unique hours:   {}'.format(X_train['Date'].dt.hour.unique()))\n",
    "print('Unique minutes: {}'.format(X_train['Date'].dt.minute.unique()))\n",
    "print('Unique seconds: {}'.format(X_train['Date'].dt.second.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform common preparation on the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform common cleaning and feature engineering tasks on datasets.\n",
    "def prep_dataset(X):\n",
    "    \n",
    "    # FEATURE ENGINEERING\n",
    "    \n",
    "    # Extract days, hours, and minutes from timestamp.\n",
    "    day = X['Date'].dt.day\n",
    "    X['Day'] = day.astype('float64')\n",
    "    \n",
    "    hour = X['Date'].dt.hour\n",
    "    X['Hour'] = hour.astype('float64')\n",
    "    \n",
    "    minute = X['Date'].dt.minute\n",
    "    X['Minute'] = minute.astype('float64')\n",
    "\n",
    "    return X\n",
    "\n",
    "X_train = prep_dataset(X_train.copy())\n",
    "\n",
    "X_test = prep_dataset(X_test.copy())\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop columns that won't be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused columns from datasets.\n",
    "def drop_unused(X):\n",
    "    \n",
    "    # This column been divided up into multiple columns.\n",
    "    X = X.drop(['Date'], axis = 1)\n",
    "    \n",
    "    return X\n",
    "\n",
    "print('Columns before drop:\\n\\n{}\\n'.format(list(X_train.columns)))\n",
    "X_train = drop_unused(X_train.copy())\n",
    "print('Columns after drop:\\n\\n{}\\n'.format(list(X_train.columns)))\n",
    "\n",
    "X_test = drop_unused(X_test.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    result = X.copy()\n",
    "    \n",
    "    for feature in X.columns:\n",
    "        result[feature] = (X[feature] - X[feature].mean()) / X[feature].std()  # z-score formula.\n",
    "        \n",
    "    return result\n",
    "\n",
    "X_train = standardize(X_train)\n",
    "\n",
    "X_test = standardize(X_test)\n",
    "\n",
    "print('The features have been standardized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('float_format', '{:.2f}'.format): \n",
    "    print(X_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (2),\n",
    "                    activation = 'relu',\n",
    "                    solver = 'adam',\n",
    "                    alpha = 0.0001,\n",
    "                    learning_rate_init = 0.001,\n",
    "                    max_iter = 500,\n",
    "                    tol = 1e-4,\n",
    "                    n_iter_no_change = 10,\n",
    "                    verbose = True,\n",
    "                    random_state = 87)\n",
    "\n",
    "mlp.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "score = mlp.score(X_test, y_test)\n",
    "\n",
    "print('Accuracy: {:.0f}%'.format(score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the loss minimization through gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(model):\n",
    "    plt.plot(model.loss_curve_)\n",
    "    plt.title('GD Loss Minimization')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "plot_loss(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_diagram(X, y, model, show_weights):\n",
    "\n",
    "    # Create structure of network from dataset shapes and hidden layer sizes.\n",
    "    nn_struct = np.hstack(([X.shape[1]], np.asarray(model.hidden_layer_sizes), [y.shape[1]]))\n",
    "\n",
    "    # Only plot weights if specified.\n",
    "    if show_weights == True:\n",
    "        network = VisNN.DrawNN(nn_struct, model.coefs_)\n",
    "    else:\n",
    "        network = VisNN.DrawNN(nn_struct)\n",
    "        \n",
    "    network.draw()\n",
    "    \n",
    "nn_diagram(X_train, y_train, mlp, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve the neuron weights and bias terms and redraw the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Weights between input layer and hidden layer:')\n",
    "print(mlp.coefs_[0], '\\n')\n",
    "print('Weights between hidden layer and output layer:')\n",
    "print(mlp.coefs_[1], '\\n')\n",
    "print('Bias terms between input layer and hidden layer:')\n",
    "print(mlp.intercepts_[0], '\\n')\n",
    "print('Bias terms between hidden layer and output layer:')\n",
    "print(mlp.intercepts_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_diagram(X_train, y_train, mlp, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit an MLP model using grid search with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "mlp = MLPClassifier(alpha = 0.0001,\n",
    "                    learning_rate_init = 0.001,\n",
    "                    max_iter = 500,\n",
    "                    tol = 1e-4,\n",
    "                    n_iter_no_change = 10,\n",
    "                    random_state = 87)\n",
    "\n",
    "grid = {'hidden_layer_sizes': [(5), (6)],\n",
    "        'activation': ['logistic', 'tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam']}\n",
    "\n",
    "search = GridSearchCV(mlp, param_grid = grid, scoring = 'accuracy', cv = 5, iid = False)\n",
    "\n",
    "start = time()\n",
    "search.fit(X_train, np.ravel(y_train))\n",
    "end = time()\n",
    "train_time = (end - start)\n",
    "\n",
    "print('Grid search took {:.2f} seconds to find an optimal fit.'.format(train_time))\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = search.score(X_test, y_test)\n",
    "\n",
    "print('Accuracy: {:.0f}%'.format(score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the loss minimization of the optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the network structure of the optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_diagram(X_train, y_train, search.best_estimator_, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the model's predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example predictions with the test data.\n",
    "results = data_raw_test.copy()\n",
    "results['PredictedOccupancy'] = search.predict(X_test)\n",
    "results.rename(columns = {'Occupancy': 'ActualOccupancy'}, inplace = True)  # Clarify ground truth column.\n",
    "results.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
